{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fa9951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed9c9f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "base_dir = \"../results/experiment-001\"\n",
    "# Find all folders (models + control)\n",
    "model_folders = [f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de9eb49",
   "metadata": {},
   "source": [
    "## Trapezoid Rule\n",
    "\n",
    "We analyze each csv for each model. We use the Trapezoid rule to calculate the Total Energy (Joules) for a specific trial and we repeat 30 Times: Do this for all 30 CSVs in that model's folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143deebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will store every single trial's final result in this list\n",
    "all_trials_data = []\n",
    "\n",
    "print(\"Processing all CSV files using the Trapezoid Rule...\")\n",
    "\n",
    "for folder in model_folders:\n",
    "    # Find all 30 CSVs in this folder\n",
    "    csv_pattern = os.path.join(base_dir, folder, \"trial-*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Failsafe: check if the GPU power column exists\n",
    "        if 'GPU0_POWER (mWatts)' not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Convert timestamp to elapsed Seconds (starting from 0)\n",
    "        time_sec = (df['Time'] - df['Time'].min()) / 1000.0\n",
    "        \n",
    "        # Convert milliWatts to standard Watts\n",
    "        power_watts = df['GPU0_POWER (mWatts)'] / 1000.0\n",
    "        \n",
    "        # Calculate Area Under the Curve (Total Joules)\n",
    "        total_energy_joules = np.trapezoid(y=power_watts, x=time_sec)\n",
    "        \n",
    "        # Get the total duration of this specific trial\n",
    "        total_time_seconds = time_sec.max()\n",
    "        \n",
    "        # Store the calculated metrics for THIS trial\n",
    "        all_trials_data.append({\n",
    "            'Model': folder,\n",
    "            'Trial_Name': os.path.basename(file),\n",
    "            'Total_Energy_Joules': total_energy_joules,\n",
    "            'Total_Time_Seconds': total_time_seconds\n",
    "        })\n",
    "\n",
    "# Convert the master list into a DataFrame\n",
    "df_results = pd.DataFrame(all_trials_data)\n",
    "\n",
    "print(f\"\\nSuccessfully processed {len(df_results)} trials.\")\n",
    "print(\"\\nHere is a sneak peek of the dataset:\")\n",
    "display(df_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4da2a",
   "metadata": {},
   "source": [
    "## Anomalies Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d08f9db",
   "metadata": {},
   "source": [
    "Running the following block of code, we identified that 5 out of our 210 executions were anomalous. Four trials reached our 60-second hardware timeout limit and were forcefully aborted, resulting in extreme energy spikes. One trial probably experienced a software crash (terminating in 0.6 seconds). Following standard data-cleaning procedures, these outliers were discarded to prevent skewed distributions. Furthermore, sensor data confirmed that the GPU temperature never exceeded 66°C, validating that our cooling and resting strategy effectively prevented thermal throttling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c14515",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== OUTLIER DETECTION REPORT ===\\n\")\n",
    "valid_trials = []\n",
    "\n",
    "# Flag and filter outliers per model\n",
    "for model in df_results['Model'].unique():\n",
    "    model_df = df_results[df_results['Model'] == model].copy()\n",
    "    \n",
    "    # Calculate Mean and Standard Deviation for Time\n",
    "    mean_time = model_df['Total_Time_Seconds'].mean()\n",
    "    std_time = model_df['Total_Time_Seconds'].std()\n",
    "    \n",
    "    # We use different rules for the control group vs the AI models\n",
    "    if model == 'control':\n",
    "        # For the control group, we filter out extreme crashes and anomalies > 3 standard deviations\n",
    "        outlier_condition = (\n",
    "            (model_df['Total_Time_Seconds'] < 1.0) | \n",
    "            (np.abs(model_df['Total_Time_Seconds'] - mean_time) > 3 * std_time)\n",
    "        )\n",
    "    else:\n",
    "        # For the AI models, we filter out crashes, timeouts (>= 59s), and anomalies > 3 standard deviations\n",
    "        outlier_condition = (\n",
    "            (model_df['Total_Time_Seconds'] < 1.0) | \n",
    "            (model_df['Total_Time_Seconds'] >= 59.0) |\n",
    "            (np.abs(model_df['Total_Time_Seconds'] - mean_time) > 3 * std_time)\n",
    "        )\n",
    "    \n",
    "    outliers = model_df[outlier_condition]\n",
    "    clean_data = model_df[~outlier_condition]\n",
    "    valid_trials.append(clean_data)\n",
    "    \n",
    "    if not outliers.empty and model != 'control':\n",
    "        print(f\"⚠️ {model} had {len(outliers)} anomalous runs discarded:\")\n",
    "        for _, row in outliers.iterrows():\n",
    "            print(f\"   - {row['Trial_Name']}: Time = {row['Total_Time_Seconds']:.2f}s, Energy = {row['Total_Energy_Joules']:.0f} J\")\n",
    "        print()\n",
    "\n",
    "# Create your final, clean dataset!\n",
    "clean_df = pd.concat(valid_trials)\n",
    "\n",
    "print(f\"✅ Data cleaning complete. Retained {len(clean_df)} valid trials out of {len(df_results)} total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e5d85",
   "metadata": {},
   "source": [
    "### Checking Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64953bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scanning temperature sensors across all trials...\")\n",
    "\n",
    "max_temps = {}\n",
    "global_max = 0\n",
    "\n",
    "# Loop through every single CSV and extract the maximum temperature\n",
    "for folder in model_folders:\n",
    "    csv_files = glob.glob(os.path.join(base_dir, folder, \"trial-*.csv\"))\n",
    "    folder_max = 0\n",
    "    \n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            if 'GPU0_TEMPERATURE' in df.columns:\n",
    "                trial_max = df['GPU0_TEMPERATURE'].max()\n",
    "                \n",
    "                # Update the max for this specific model\n",
    "                if trial_max > folder_max:\n",
    "                    folder_max = trial_max\n",
    "                \n",
    "                # Update the absolute global max\n",
    "                if trial_max > global_max:\n",
    "                    global_max = trial_max\n",
    "        except Exception:\n",
    "            continue\n",
    "            \n",
    "    max_temps[folder] = folder_max\n",
    "\n",
    "# Print the final conclusive statement\n",
    "print(f\"\\nThe absolute Maximum GPU Temperature recorded was {global_max}°C.\")\n",
    "if global_max <= 66:\n",
    "    print(\"Conclusion Validated: Thermal throttling was successfully prevented.\")\n",
    "\n",
    "# Visual proof for the report\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(max_temps.keys(), max_temps.values(), color='#e74c3c', edgecolor='black', alpha=0.8)\n",
    "\n",
    "plt.axhline(y=85, color='red', linestyle='--', linewidth=2, label='Typical Thermal Throttling Limit (85°C)')\n",
    "\n",
    "plt.title(\"Peak GPU Temperature per AI Model During Execution\", fontsize=15, fontweight='bold')\n",
    "plt.ylabel(\"Maximum Temperature (°C)\", fontsize=12)\n",
    "plt.xticks(rotation=35, ha='right', fontsize=10)\n",
    "\n",
    "plt.ylim(0, 100) \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Add the exact temperature number on top of each bar\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 1.5, f\"{int(yval)}°C\", ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4572ce",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbc8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = clean_df[clean_df['Model'] != 'control'].copy()\n",
    "\n",
    "# Shorten the names to make the X-axis readable\n",
    "plot_df['Model_Short'] = plot_df['Model'].str.replace('deepseek-r1_8b-llama-distill-', 'DS-')\n",
    "plot_df['Model_Short'] = plot_df['Model_Short'].str.replace('llama3.1_8b-instruct-', 'Llama-')\n",
    "\n",
    "# We group 4-bit together, then 8-bit, then 16-bit\n",
    "quantization_order = [\n",
    "    'Llama-q4_K_M', 'DS-q4_K_M', \n",
    "    'Llama-q8_0',   'DS-q8_0', \n",
    "    'Llama-fp16',   'DS-fp16'\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create the Violin Plot, passing the new order\n",
    "sns.violinplot(\n",
    "    x='Model_Short', \n",
    "    y='Total_Energy_Joules', \n",
    "    data=plot_df, \n",
    "    order=quantization_order,\n",
    "    inner=None, \n",
    "    color=\"lightgray\", \n",
    "    linewidth=1.5,\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "# Overlay the Box Plot, passing the SAME order so they align perfectly\n",
    "sns.boxplot(\n",
    "    x='Model_Short', \n",
    "    y='Total_Energy_Joules', \n",
    "    data=plot_df, \n",
    "    order=quantization_order, \n",
    "    width=0.2, \n",
    "    boxprops={'zorder': 2, 'alpha': 0.9},\n",
    "    palette=\"Set2\" \n",
    ")\n",
    "\n",
    "plt.title(\"Energy Consumption Distribution (Grouped by Quantization)\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Model Version (4-bit, 8-bit, 16-bit)\", fontsize=12)\n",
    "plt.ylabel(\"Total Energy (Joules)\", fontsize=12)\n",
    "\n",
    "plt.xticks(rotation=20, ha='right', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbced89",
   "metadata": {},
   "source": [
    "## Shapiro - Wilk test & T-test for Normal or Mann-Whitney U Test for not Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf58b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pairs we want to compare in a list\n",
    "model_pairs = [\n",
    "    ('llama3.1_8b-instruct-q4_K_M', 'deepseek-r1_8b-llama-distill-q4_K_M', '4-bit'),\n",
    "    ('llama3.1_8b-instruct-q8_0', 'deepseek-r1_8b-llama-distill-q8_0', '8-bit'),\n",
    "    ('llama3.1_8b-instruct-fp16', 'deepseek-r1_8b-llama-distill-fp16', '16-bit')\n",
    "]\n",
    "\n",
    "for llama_name, ds_name, precision in model_pairs:\n",
    "    print(f\"======================================================\")\n",
    "    print(f\"=== STATISTICAL SIGNIFICANCE TEST: {precision.upper()} MODELS ===\")\n",
    "    print(f\"======================================================\\n\")\n",
    "    \n",
    "    # Isolate the data for the current pair\n",
    "    llama_data = clean_df[clean_df['Model'] == llama_name]['Total_Energy_Joules']\n",
    "    ds_data = clean_df[clean_df['Model'] == ds_name]['Total_Energy_Joules']\n",
    "    \n",
    "    # 1. The Normality Test (Shapiro-Wilk)\n",
    "    stat_l, p_l = stats.shapiro(llama_data)\n",
    "    stat_d, p_d = stats.shapiro(ds_data)\n",
    "    \n",
    "    print(f\"Shapiro-Wilk p-value (Llama): {p_l:.5f}\")\n",
    "    print(f\"Shapiro-Wilk p-value (DeepSeek): {p_d:.5f}\")\n",
    "    \n",
    "    # 2. Choose the right test dynamically based on the Professor's rules\n",
    "    if p_l > 0.05 and p_d > 0.05:\n",
    "        print(\"-> Both p-values are > 0.05. The data IS normally distributed.\")\n",
    "        print(\"-> Action: Using the Parametric Welch's T-Test.\\n\")\n",
    "        stat_val, p_value = stats.ttest_ind(llama_data, ds_data, equal_var=False)\n",
    "        test_name = \"Welch's T-Test\"\n",
    "        \n",
    "        # Use Means for the Effect Size since data is normal\n",
    "        val_llama = llama_data.mean()\n",
    "        val_ds = ds_data.mean()\n",
    "        metric_name = \"Mean\"\n",
    "    else:\n",
    "        print(\"-> At least one p-value is < 0.05. The data is NOT normally distributed.\")\n",
    "        print(\"-> Action: Using the Non-Parametric Mann-Whitney U Test.\\n\")\n",
    "        stat_val, p_value = stats.mannwhitneyu(llama_data, ds_data, alternative='two-sided')\n",
    "        test_name = \"Mann-Whitney U Test\"\n",
    "        \n",
    "        # Use Medians for the Effect Size since data is heavily skewed\n",
    "        val_llama = llama_data.median()\n",
    "        val_ds = ds_data.median()\n",
    "        metric_name = \"Median\"\n",
    "        \n",
    "    # 3. Print Final Significance\n",
    "    print(f\"{test_name} p-value: {p_value:.5e}\")\n",
    "    if p_value < 0.05:\n",
    "        print(f\"-> CONCLUSION: The energy difference between the {precision} models is STATISTICALLY SIGNIFICANT.\\n\")\n",
    "    else:\n",
    "        print(f\"-> CONCLUSION: The difference is not statistically significant (just random noise).\\n\")\n",
    "\n",
    "    # 4. Effect Size (Percentage Change)\n",
    "    percent_diff = ((val_llama - val_ds) / val_llama) * 100\n",
    "\n",
    "    print(\"=== EFFECT SIZE (PERCENTAGE CHANGE) ===\")\n",
    "    print(f\"Llama {metric_name} Energy: {val_llama:.1f} Joules\")\n",
    "    print(f\"DeepSeek {metric_name} Energy: {val_ds:.1f} Joules\")\n",
    "    \n",
    "    if percent_diff > 0:\n",
    "        print(f\"✅ RESULT: DeepSeek is {percent_diff:.1f}% MORE ENERGY EFFICIENT than Llama.\\n\\n\")\n",
    "    else:\n",
    "        print(f\"✅ RESULT: Llama is {abs(percent_diff):.1f}% MORE ENERGY EFFICIENT than DeepSeek.\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc6b2a",
   "metadata": {},
   "source": [
    "#### WITHIN-FAMILY QUANTIZATION EFFECTIVENESS ANALYSIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ada2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# WITHIN-FAMILY QUANTIZATION EFFECTIVENESS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUANTIZATION EFFECTIVENESS: DOES SMALLER = LESS ENERGY?\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Define families\n",
    "families = {\n",
    "    'Llama': ['llama3.1_8b-instruct-fp16', 'llama3.1_8b-instruct-q8_0', 'llama3.1_8b-instruct-q4_K_M'],\n",
    "    'DeepSeek': ['deepseek-r1_8b-llama-distill-fp16', 'deepseek-r1_8b-llama-distill-q8_0', 'deepseek-r1_8b-llama-distill-q4_K_M']\n",
    "}\n",
    "\n",
    "quantization_results = []\n",
    "\n",
    "for family_name, models in families.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"   {family_name.upper()} FAMILY\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Extract data\n",
    "    fp16_data = clean_df[clean_df['Model'] == models[0]]['Total_Energy_Joules']\n",
    "    q8_data = clean_df[clean_df['Model'] == models[1]]['Total_Energy_Joules']\n",
    "    q4_data = clean_df[clean_df['Model'] == models[2]]['Total_Energy_Joules']\n",
    "    \n",
    "    # Test 1: fp16 vs q8\n",
    "    print(f\"[Test 1] {family_name} fp16 vs q8\")\n",
    "    _, p_fp16 = stats.shapiro(fp16_data)\n",
    "    _, p_q8 = stats.shapiro(q8_data)\n",
    "    \n",
    "    if p_fp16 > 0.05 and p_q8 > 0.05:\n",
    "        stat, p_val = stats.ttest_ind(fp16_data, q8_data, equal_var=False)\n",
    "        test_name = \"Welch's t-test\"\n",
    "        val_fp16, val_q8 = fp16_data.mean(), q8_data.mean()\n",
    "        metric = \"Mean\"\n",
    "    else:\n",
    "        stat, p_val = stats.mannwhitneyu(fp16_data, q8_data, alternative='two-sided')\n",
    "        test_name = \"Mann-Whitney U\"\n",
    "        val_fp16, val_q8 = fp16_data.median(), q8_data.median()\n",
    "        metric = \"Median\"\n",
    "    \n",
    "    reduction_q8 = ((val_fp16 - val_q8) / val_fp16) * 100\n",
    "    print(f\"  {metric}: fp16={val_fp16:.1f}J, q8={val_q8:.1f}J\")\n",
    "    print(f\"  Reduction: {reduction_q8:.1f}% | p={p_val:.5f} | {'✅ Significant' if p_val < 0.05 else '❌ Not significant'}\\n\")\n",
    "    \n",
    "    # Test 2: fp16 vs q4\n",
    "    print(f\"[Test 2] {family_name} fp16 vs q4\")\n",
    "    _, p_q4 = stats.shapiro(q4_data)\n",
    "    \n",
    "    if p_fp16 > 0.05 and p_q4 > 0.05:\n",
    "        stat, p_val = stats.ttest_ind(fp16_data, q4_data, equal_var=False)\n",
    "        test_name = \"Welch's t-test\"\n",
    "        val_fp16, val_q4 = fp16_data.mean(), q4_data.mean()\n",
    "        metric = \"Mean\"\n",
    "    else:\n",
    "        stat, p_val = stats.mannwhitneyu(fp16_data, q4_data, alternative='two-sided')\n",
    "        test_name = \"Mann-Whitney U\"\n",
    "        val_fp16, val_q4 = fp16_data.median(), q4_data.median()\n",
    "        metric = \"Median\"\n",
    "    \n",
    "    reduction_q4 = ((val_fp16 - val_q4) / val_fp16) * 100\n",
    "    print(f\"  {metric}: fp16={val_fp16:.1f}J, q4={val_q4:.1f}J\")\n",
    "    print(f\"  Reduction: {reduction_q4:.1f}% | p={p_val:.5f} | {'✅ Significant' if p_val < 0.05 else '❌ Not significant'}\\n\")\n",
    "    \n",
    "    # Test 3: q8 vs q4\n",
    "    print(f\"[Test 3] {family_name} q8 vs q4\")\n",
    "    \n",
    "    if p_q8 > 0.05 and p_q4 > 0.05:\n",
    "        stat, p_val = stats.ttest_ind(q8_data, q4_data, equal_var=False)\n",
    "        test_name = \"Welch's t-test\"\n",
    "        val_q8, val_q4 = q8_data.mean(), q4_data.mean()\n",
    "        metric = \"Mean\"\n",
    "    else:\n",
    "        stat, p_val = stats.mannwhitneyu(q8_data, q4_data, alternative='two-sided')\n",
    "        test_name = \"Mann-Whitney U\"\n",
    "        val_q8, val_q4 = q8_data.median(), q4_data.median()\n",
    "        metric = \"Median\"\n",
    "    \n",
    "    reduction_q8_to_q4 = ((val_q8 - val_q4) / val_q8) * 100\n",
    "    print(f\"  {metric}: q8={val_q8:.1f}J, q4={val_q4:.1f}J\")\n",
    "    print(f\"  Reduction: {reduction_q8_to_q4:.1f}% | p={p_val:.5f} | {'✅ Significant' if p_val < 0.05 else '❌ Not significant'}\\n\")\n",
    "    \n",
    "    # Store for summary\n",
    "    quantization_results.append({\n",
    "        'Family': family_name,\n",
    "        'fp16→q8': reduction_q8,\n",
    "        'fp16→q4': reduction_q4,\n",
    "        'q8→q4': reduction_q8_to_q4\n",
    "    })\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY: Energy Reduction from Quantization (%)\")\n",
    "print(\"=\"*70)\n",
    "df_quant_summary = pd.DataFrame(quantization_results)\n",
    "print(df_quant_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9250cc",
   "metadata": {},
   "source": [
    "#### HEATMAP: Energy Savings vs fp16 Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d36c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HEATMAP: Energy Savings vs fp16 Baseline\n",
    "# ============================================================================\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Prepare data for heatmap\n",
    "heatmap_data = []\n",
    "for family_name, models in families.items():\n",
    "    fp16_median = clean_df[clean_df['Model'] == models[0]]['Total_Energy_Joules'].median()\n",
    "    q8_median = clean_df[clean_df['Model'] == models[1]]['Total_Energy_Joules'].median()\n",
    "    q4_median = clean_df[clean_df['Model'] == models[2]]['Total_Energy_Joules'].median()\n",
    "    \n",
    "    savings_q8 = ((fp16_median - q8_median) / fp16_median) * 100\n",
    "    savings_q4 = ((fp16_median - q4_median) / fp16_median) * 100\n",
    "    \n",
    "    heatmap_data.append([savings_q8, savings_q4])\n",
    "\n",
    "df_heatmap = pd.DataFrame(heatmap_data, \n",
    "                          columns=['8-bit (q8)', '4-bit (q4)'],\n",
    "                          index=['Llama 3.1', 'DeepSeek-R1'])\n",
    "\n",
    "# Plot\n",
    "sns.heatmap(df_heatmap, annot=True, fmt='.1f', cmap='RdYlGn', \n",
    "            center=0, vmin=-5, vmax=75,\n",
    "            cbar_kws={'label': 'Energy Reduction (%)'}, \n",
    "            linewidths=2, linecolor='white')\n",
    "\n",
    "plt.title('Energy Savings vs FP16 Baseline', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('Model Family', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Quantization Level', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Interpretation:\")\n",
    "print(\"   - Green = Good (energy saved)\")\n",
    "print(\"   - Red = Bad (energy increased)\")\n",
    "print(\"   - Values show % reduction compared to fp16 baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff86d8e",
   "metadata": {},
   "source": [
    "#### AVERAGE POWER (W) ANALYSIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AVERAGE POWER (W) ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate average power\n",
    "clean_df['Avg_Power_W'] = clean_df['Total_Energy_Joules'] / clean_df['Total_Time_Seconds']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AVERAGE POWER CONSUMPTION (W)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "power_summary = clean_df.groupby('Model')['Avg_Power_W'].agg(['mean', 'median', 'std'])\n",
    "power_summary = power_summary.round(2)\n",
    "power_summary = power_summary.sort_values('median')\n",
    "\n",
    "print(power_summary)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_df = clean_df.copy()\n",
    "plot_df['Model_Short'] = plot_df['Model'].str.replace('deepseek-r1_8b-llama-distill-', 'DS-')\n",
    "plot_df['Model_Short'] = plot_df['Model_Short'].str.replace('llama3.1_8b-instruct-', 'Llama-')\n",
    "\n",
    "quantization_order = [\n",
    "    'Llama-q4_K_M', 'DS-q4_K_M', \n",
    "    'Llama-q8_0',   'DS-q8_0', \n",
    "    'Llama-fp16',   'DS-fp16'\n",
    "]\n",
    "\n",
    "sns.boxplot(x='Model_Short', y='Avg_Power_W', data=plot_df, \n",
    "            order=quantization_order, palette='Set2')\n",
    "\n",
    "plt.title('Average Power Consumption by Model', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Model Version', fontsize=12)\n",
    "plt.ylabel('Average Power (Watts)', fontsize=12)\n",
    "plt.xticks(rotation=20, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Lower power = gentler on hardware, better for sustained workloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01919ebe",
   "metadata": {},
   "source": [
    "#### ENERGY VS TIME TRADE-OFF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e112382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENERGY VS TIME TRADE-OFF\n",
    "# ============================================================================\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Color mapping\n",
    "color_map = {\n",
    "    'llama3.1_8b-instruct-q4_K_M': '#2ca02c',\n",
    "    'deepseek-r1_8b-llama-distill-q4_K_M': '#98df8a',\n",
    "    'llama3.1_8b-instruct-q8_0': '#ff7f0e',\n",
    "    'deepseek-r1_8b-llama-distill-q8_0': '#ffbb78',\n",
    "    'llama3.1_8b-instruct-fp16': '#d62728',\n",
    "    'deepseek-r1_8b-llama-distill-fp16': '#ff9896'\n",
    "}\n",
    "\n",
    "label_map = {\n",
    "    'llama3.1_8b-instruct-q4_K_M': 'Llama q4',\n",
    "    'deepseek-r1_8b-llama-distill-q4_K_M': 'DeepSeek q4',\n",
    "    'llama3.1_8b-instruct-q8_0': 'Llama q8',\n",
    "    'deepseek-r1_8b-llama-distill-q8_0': 'DeepSeek q8',\n",
    "    'llama3.1_8b-instruct-fp16': 'Llama fp16',\n",
    "    'deepseek-r1_8b-llama-distill-fp16': 'DeepSeek fp16'\n",
    "}\n",
    "\n",
    "for model in clean_df['Model'].unique():\n",
    "    if model == 'control':\n",
    "        continue\n",
    "    data = clean_df[clean_df['Model'] == model]\n",
    "    plt.scatter(data['Total_Time_Seconds'], data['Total_Energy_Joules'],\n",
    "               label=label_map[model], color=color_map[model], \n",
    "               s=80, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "plt.xlabel('Execution Time (seconds)', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Total Energy (Joules)', fontsize=13, fontweight='bold')\n",
    "plt.title('Energy vs Time Trade-off\\n(Lower-left = Best: Fast + Efficient)', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.legend(loc='upper left', framealpha=0.9, fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Interpretation:\")\n",
    "print(\"   - Lower-left corner = BEST (fast + energy-efficient)\")\n",
    "print(\"   - Upper-right corner = WORST (slow + energy-hungry)\")\n",
    "print(\"   - Check if quantization makes models faster or slower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8106abf5",
   "metadata": {},
   "source": [
    "#### ENERGY DELAY PRODUCT (EDP) - Overall Efficiency Metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b8e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENERGY DELAY PRODUCT (EDP) - Overall Efficiency Metric\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate EDP (penalizes slow execution)\n",
    "clean_df['EDP'] = clean_df['Total_Energy_Joules'] * (clean_df['Total_Time_Seconds'] ** 2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENERGY DELAY PRODUCT (EDP) - Lower = Better\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "edp_summary = clean_df.groupby('Model')['EDP'].agg(['mean', 'median', 'std'])\n",
    "edp_summary = edp_summary.sort_values('median')\n",
    "edp_summary = edp_summary.round(1)\n",
    "\n",
    "print(edp_summary)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_df = clean_df.copy()\n",
    "plot_df['Model_Short'] = plot_df['Model'].str.replace('deepseek-r1_8b-llama-distill-', 'DS-')\n",
    "plot_df['Model_Short'] = plot_df['Model_Short'].str.replace('llama3.1_8b-instruct-', 'Llama-')\n",
    "\n",
    "sns.boxplot(x='Model_Short', y='EDP', data=plot_df, \n",
    "            order=quantization_order, palette='coolwarm')\n",
    "\n",
    "plt.title('Energy Delay Product (EDP)\\nLower = Better Overall Efficiency', \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Model Version', fontsize=12)\n",
    "plt.ylabel('EDP (J·s²)', fontsize=12)\n",
    "plt.xticks(rotation=20, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ EDP = Energy × Time²\")\n",
    "print(\"   Penalizes models that are slow, even if energy-efficient\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
